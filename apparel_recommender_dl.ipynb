{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fabe192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D\n",
    "from tensorflow.keras import Sequential\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn import set_config\n",
    "from numpy.linalg import norm\n",
    "import pickle\n",
    "#resnet model\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50,preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd296c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating instance (object) of the model\n",
    "model = ResNet50(weights='imagenet',include_top=False,input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b6606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disabling training the model \n",
    "model.trainable= False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b94a931",
   "metadata": {},
   "source": [
    "#### Why do we update the last layer of the model?\n",
    "while finding the images that are closest to the one uploaded by the user, we need to calculate the eucledian distance of the vector created from the uploaded image with every single vector created from the images in the database. To reduce the complexity and hence the time required for the same, we need to reduce the dimensionality of the vector of features produced by the model. By default, the model produces a matrix of 4 dimensions, the GlobalMaxPooling2D layer reduces that to 2. The model scans the image, the CNN layers go through each individual pixel, try to analyse the patterns and ultimately produce a matrix of 2048 features which the model thinks best seperates the features apart from one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9a359f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the top layer of the model and replacing it with GlobalMaxPooling2D layer\n",
    "model = Sequential([\n",
    "    model,  #uptil here the model is the same as that which we imported\n",
    "    GlobalMaxPooling2D()   #the new GlobalMaxPooling2D layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ceba323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
      "                                                                 \n",
      " global_max_pooling2d (Glob  (None, 2048)              0         \n",
      " alMaxPooling2D)                                                 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23587712 (89.98 MB)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 23587712 (89.98 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e0be92",
   "metadata": {},
   "source": [
    "As we can see, the output shape of the GlobalMaxPooling2D() layer is a 2D matrix instead of 4 which is what we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6b62a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img('images/1163.jpg',target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d556d2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'>\n"
     ]
    }
   ],
   "source": [
    "print(type(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "022688b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = image.img_to_array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34ab0fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402efe91",
   "metadata": {},
   "source": [
    "Since the keras Resnet50 model takes in a 4D matrix as an input instead of a 3D matrix, where the extra dimension refers to batch size; we have to convert this 3D matrix image into a 4D one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d816eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding an extra dimension to the image array\n",
    "reshaped_img = np.expand_dims(img_array,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c1b7fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 224, 224, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43917a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[255., 255., 255.],\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.],\n",
       "         ...,\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.]],\n",
       "\n",
       "        [[255., 255., 255.],\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.],\n",
       "         ...,\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.]],\n",
       "\n",
       "        [[255., 255., 255.],\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.],\n",
       "         ...,\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[255., 255., 255.],\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.],\n",
       "         ...,\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.]],\n",
       "\n",
       "        [[255., 255., 255.],\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.],\n",
       "         ...,\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.]],\n",
       "\n",
       "        [[255., 255., 255.],\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.],\n",
       "         ...,\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.],\n",
       "         [255., 255., 255.]]]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "799a3ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing the array slightly to make sure it comes in the format of the input of the model.\n",
    "preprocessed_img_arr = preprocess_input(reshaped_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9aec3a67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[151.061  , 138.22101, 131.32   ],\n",
       "         [151.061  , 138.22101, 131.32   ],\n",
       "         [151.061  , 138.22101, 131.32   ],\n",
       "         ...,\n",
       "         [151.061  , 138.22101, 131.32   ],\n",
       "         [151.061  , 138.22101, 131.32   ],\n",
       "         [151.061  , 138.22101, 131.32   ]],\n",
       "\n",
       "        [[151.061  , 138.22101, 131.32   ],\n",
       "         [151.061  , 138.22101, 131.32   ],\n",
       "         [151.061  , 138.22101, 131.32   ],\n",
       "         ...,\n",
       "         [151.061  , 138.22101, 131.32   ],\n",
       "         [151.061  , 138.22101, 131.32   ],\n",
       "         [151.061  , 138.22101, 131.32   ]],\n",
       "\n",
       "        [[151.061  , 138.22101, 131.32   ],\n",
       "         [151.061  , 138.22101, 131.32   ],\n",
       "         [151.061  , 138.22101, 131.32   ],\n",
       "         ...,\n",
       "         [151.061  , 138.22101, 131.32   ],\n",
       "         [151.061  , 138.22101, 131.32   ],\n",
       "         [151.061  , 138.22101, 131.32   ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[151.061  , 138.22101, 131.32   ],\n",
       "         [151.061  , 138.22101, 131.32   ],\n",
       "         [151.061  , 138.22101, 131.32   ],\n",
       "         ...,\n",
       "         [151.061  , 138.22101, 131.32   ],\n",
       "         [151.061  , 138.22101, 131.32   ],\n",
       "         [151.061  , 138.22101, 131.32   ]],\n",
       "\n",
       "        [[151.061  , 138.22101, 131.32   ],\n",
       "         [151.061  , 138.22101, 131.32   ],\n",
       "         [151.061  , 138.22101, 131.32   ],\n",
       "         ...,\n",
       "         [151.061  , 138.22101, 131.32   ],\n",
       "         [151.061  , 138.22101, 131.32   ],\n",
       "         [151.061  , 138.22101, 131.32   ]],\n",
       "\n",
       "        [[151.061  , 138.22101, 131.32   ],\n",
       "         [151.061  , 138.22101, 131.32   ],\n",
       "         [151.061  , 138.22101, 131.32   ],\n",
       "         ...,\n",
       "         [151.061  , 138.22101, 131.32   ],\n",
       "         [151.061  , 138.22101, 131.32   ],\n",
       "         [151.061  , 138.22101, 131.32   ]]]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_img_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5d4fc4",
   "metadata": {},
   "source": [
    "This **preprocessed_img_arr** becomes the input for the model which can be used to predict the 2048 features and return a vector of the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4fcb8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_image_to_input(path):\n",
    "    img = image.load_img(path,target_size=(224,224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    reshaped_img = np.expand_dims(img_array,axis=0)\n",
    "    preprocessed_img_arr = preprocess_input(reshaped_img)\n",
    "    return preprocessed_img_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1a44f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_image_array_to1D(array):\n",
    "    array = array.flatten()\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "085cc600",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = reshape_image_to_input('images/1540.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18c01511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 224, 224, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1058c9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 733ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79e8daf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2038</th>\n",
       "      <th>2039</th>\n",
       "      <th>2040</th>\n",
       "      <th>2041</th>\n",
       "      <th>2042</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2046</th>\n",
       "      <th>2047</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.585339</td>\n",
       "      <td>14.773425</td>\n",
       "      <td>1.597528</td>\n",
       "      <td>3.998313</td>\n",
       "      <td>4.783364</td>\n",
       "      <td>2.891763</td>\n",
       "      <td>13.972631</td>\n",
       "      <td>5.486446</td>\n",
       "      <td>0.094314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340045</td>\n",
       "      <td>4.152175</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.696899</td>\n",
       "      <td>2.755452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.122338</td>\n",
       "      <td>5.244023</td>\n",
       "      <td>10.181306</td>\n",
       "      <td>4.552249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 2048 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0          1         2         3         4         5          6     \\\n",
       "0  2.585339  14.773425  1.597528  3.998313  4.783364  2.891763  13.972631   \n",
       "\n",
       "       7         8     9     ...      2038      2039  2040       2041  \\\n",
       "0  5.486446  0.094314   0.0  ...  0.340045  4.152175   0.0  17.696899   \n",
       "\n",
       "       2042  2043      2044      2045       2046      2047  \n",
       "0  2.755452   0.0  8.122338  5.244023  10.181306  4.552249  \n",
       "\n",
       "[1 rows x 2048 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e45a5e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = ColumnTransformer([\n",
    "#     ('scaler',MinMaxScaler(feature_range=(0.1, 0.9)),slice(0,2048))\n",
    "# ])\n",
    "def scaler_func(data):\n",
    "    return data/norm(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27a62b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.5853388, 14.773425 ,  1.5975276, ...,  5.2440233, 10.181306 ,\n",
       "        4.5522494], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_pred=flatten_image_array_to1D(prediction)\n",
    "flat_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "215ce4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = scaler_func(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dde17939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00940838, 0.05376237, 0.00581361, ..., 0.01908367, 0.03705106,\n",
       "        0.01656621]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c9be2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00940838, 0.05376237, 0.00581361, ..., 0.01908367, 0.03705106,\n",
       "       0.01656621], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_arr = flatten_image_array_to1D(arr)\n",
    "flat_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d4b319",
   "metadata": {},
   "source": [
    "### Creating pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6828e213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first transformer\n",
    "image_array_transformer = FunctionTransformer(func=reshape_image_to_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8190a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#second transformer\n",
    "def pred_with_model(data):\n",
    "    data_pred = model.predict(data)\n",
    "    return data_pred\n",
    "\n",
    "model_transformer = FunctionTransformer(func=pred_with_model)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9dad75b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#third transformer\n",
    "scaler_f = FunctionTransformer(func=scaler_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffdff732",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fourth transformer \n",
    "flatten = FunctionTransformer(func=flatten_image_array_to1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02a882ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "146baecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "model_pipeline = Pipeline([\n",
    "    ('image_to_array',image_array_transformer),\n",
    "    ('model',model_transformer),\n",
    "    ('scaler',scaler_f),\n",
    "    ('flatten',flatten)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4b58ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 63ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted_arr = model_pipeline.fit_transform('images/1551.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3e3abea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02201655, 0.00789968, 0.00425501, ..., 0.03311152, 0.02338822,\n",
       "       0.        ], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50af0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the pipeline\n",
    "pickle.dump(model_pipeline,open('model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7223c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3877fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3428d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c659743c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55a05dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc504bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eb35c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0ec6d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0338e257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2878c8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d314788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c9f4f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
